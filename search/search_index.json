{
    "docs": [
        {
            "location": "/", 
            "text": "Overview\n\n\nXXX", 
            "title": "Home"
        }, 
        {
            "location": "/#overview", 
            "text": "XXX", 
            "title": "Overview"
        }, 
        {
            "location": "/components/", 
            "text": "Components\n\n\n\n\nWhen fully deployed, the EBMeDS solution includes the components pictured above. Each component is a Docker container, inside a Docker Swarm. Any of these components can be replicated across several machines, as performance and availability needs dictate. Docker Swarm performs an automatic round-robin load balancing on every network request done to any container with multiple instances.\n\n\napi-gateway\n\n\nGithub: \n\n\nThe API gateway is the access point from the outside world. It mostly acts as a request broker, forwarding requests to the appropriate containers, usually the engine. At the moment, it also provides the translation services between FHIR and the EBMeDS native XML format. Another output option is a custom JSON format used by some EBMeDS-connected apps.\n\n\n\n\n\n\nInput: FHIR requests, EBMeDS XML requests.\n\n\n\n\n\n\nOutput: FHIR, EBMeDS XML, custom JSON formats.\n\n\n\n\n\n\nengine\n\n\nThe main service, performing most of the calculations when performing decision support. Takes patient XML data as an input, and outputs data to aid in clinical decision making. Most notably, text-based reminder messages. Also produces\n\n\n\n\n\n\nInput: EBMeDS XML\n\n\n\n\n\n\nOutput: EBMeDS XML, custom JSON formats\n\n\n\n\n\n\ncoaching\n\n\nAn ODA-specific container, may or may not be present in the future. A simple REST interface providing access to coaching programs produced by Duodecim. These programs are given in FHIR STU3 form, and contain a number of messages that are to be sent to a patient at set times to aid in e.g. weight loss, cutting down on alcohol consumtion etc.\n\n\n\n\n\n\nInput: HTTP REST requests.\n\n\n\n\n\n\nOutput: FHIR\n\n\n\n\n\n\ndiagnosis-specific-view\n\n\nA UI providing a specialised view of the results obtained from the engine, for a specific patient. This container works like a kind of proxy to the engine: instead of sending the XML with patient data to the engine, it is sent to this container. The request is sent onward to the engine with some special flags, making the engine produce a specialised JSON format, that this container renders as HTML for the user. The JSON can also be sent directly to the user, should he want to build his own UI.\n\n\n\n\n\n\nInput: EBMeDS XML\n\n\n\n\n\n\nOutput: HTML\n\n\n\n\n\n\ncomprehensive-medication-review\n\n\nSimilar to diagnosis-specific-view, this is another specialised UI view, focusing on medication.\n\n\n\n\n\n\nInput: EBMeDS XML\n\n\n\n\n\n\nOutput: HTML\n\n\n\n\n\n\nelasticsearch\n\n\nA standard Elasticsearch container, i.e. a database. Used for logging by all other containers (via logstash). Also, the engine saves request/response pairs to a separate index for debugging and statistics.\n\n\nIndices\n\n\n\n\nlogstash-*\n: app logs from all other containers\n\n\nengine-*\n: request/response messages\n\n\n\n\nlogstash\n\n\nA standard Logstash container. Logs from all other containers are sent here, where they are queued and tagged with some extra metadata.\n\n\nkibana\n\n\nA standard Kibana container. Kibana works as a web UI for inspecting logs or any other elasticsearch data. At the moment Kibana in EBMeDS is only geared towards use by system administrators and developers, but there is some demand for users to be able to access their own logs. It should be noted that with standard settings, Elasticsearch and Kibana have no user or namespace support, everything is global. This can be changed by getting an X-Pack license, which is costly.\n\n\nSupporting tools\n\n\nThere are a number of tools external to the EBMeDS deployment that is used primarily by Duodecim to produce content. These are hosted by Duodecim.\n\n\nScript editor\n\n\nURL: \n\n\nWeb-based UI for editing engine scripts (i.e. rulesets). Individual scripts can be set to apply to e.g. certain organizations, certain countries/languages or certain events. The text reminders are also defined here, as well as their translations.\n\n\nThis script editor will be replaced by a new version soon.\n\n\nCompilation\n\n\nThe scripts and their accompanying data is compiled from the script editor into text files that can be read by the engine. This compilation process also includes \"numerical\" medical data, i.e. databases of drug interactions etc. Some of this data is bought from other companies, some of it is produced in-house.", 
            "title": "Components"
        }, 
        {
            "location": "/components/#components", 
            "text": "When fully deployed, the EBMeDS solution includes the components pictured above. Each component is a Docker container, inside a Docker Swarm. Any of these components can be replicated across several machines, as performance and availability needs dictate. Docker Swarm performs an automatic round-robin load balancing on every network request done to any container with multiple instances.", 
            "title": "Components"
        }, 
        {
            "location": "/components/#api-gateway", 
            "text": "Github:   The API gateway is the access point from the outside world. It mostly acts as a request broker, forwarding requests to the appropriate containers, usually the engine. At the moment, it also provides the translation services between FHIR and the EBMeDS native XML format. Another output option is a custom JSON format used by some EBMeDS-connected apps.    Input: FHIR requests, EBMeDS XML requests.    Output: FHIR, EBMeDS XML, custom JSON formats.", 
            "title": "api-gateway"
        }, 
        {
            "location": "/components/#engine", 
            "text": "The main service, performing most of the calculations when performing decision support. Takes patient XML data as an input, and outputs data to aid in clinical decision making. Most notably, text-based reminder messages. Also produces    Input: EBMeDS XML    Output: EBMeDS XML, custom JSON formats", 
            "title": "engine"
        }, 
        {
            "location": "/components/#coaching", 
            "text": "An ODA-specific container, may or may not be present in the future. A simple REST interface providing access to coaching programs produced by Duodecim. These programs are given in FHIR STU3 form, and contain a number of messages that are to be sent to a patient at set times to aid in e.g. weight loss, cutting down on alcohol consumtion etc.    Input: HTTP REST requests.    Output: FHIR", 
            "title": "coaching"
        }, 
        {
            "location": "/components/#diagnosis-specific-view", 
            "text": "A UI providing a specialised view of the results obtained from the engine, for a specific patient. This container works like a kind of proxy to the engine: instead of sending the XML with patient data to the engine, it is sent to this container. The request is sent onward to the engine with some special flags, making the engine produce a specialised JSON format, that this container renders as HTML for the user. The JSON can also be sent directly to the user, should he want to build his own UI.    Input: EBMeDS XML    Output: HTML", 
            "title": "diagnosis-specific-view"
        }, 
        {
            "location": "/components/#comprehensive-medication-review", 
            "text": "Similar to diagnosis-specific-view, this is another specialised UI view, focusing on medication.    Input: EBMeDS XML    Output: HTML", 
            "title": "comprehensive-medication-review"
        }, 
        {
            "location": "/components/#elasticsearch", 
            "text": "A standard Elasticsearch container, i.e. a database. Used for logging by all other containers (via logstash). Also, the engine saves request/response pairs to a separate index for debugging and statistics.", 
            "title": "elasticsearch"
        }, 
        {
            "location": "/components/#indices", 
            "text": "logstash-* : app logs from all other containers  engine-* : request/response messages", 
            "title": "Indices"
        }, 
        {
            "location": "/components/#logstash", 
            "text": "A standard Logstash container. Logs from all other containers are sent here, where they are queued and tagged with some extra metadata.", 
            "title": "logstash"
        }, 
        {
            "location": "/components/#kibana", 
            "text": "A standard Kibana container. Kibana works as a web UI for inspecting logs or any other elasticsearch data. At the moment Kibana in EBMeDS is only geared towards use by system administrators and developers, but there is some demand for users to be able to access their own logs. It should be noted that with standard settings, Elasticsearch and Kibana have no user or namespace support, everything is global. This can be changed by getting an X-Pack license, which is costly.", 
            "title": "kibana"
        }, 
        {
            "location": "/components/#supporting-tools", 
            "text": "There are a number of tools external to the EBMeDS deployment that is used primarily by Duodecim to produce content. These are hosted by Duodecim.", 
            "title": "Supporting tools"
        }, 
        {
            "location": "/components/#script-editor", 
            "text": "URL:   Web-based UI for editing engine scripts (i.e. rulesets). Individual scripts can be set to apply to e.g. certain organizations, certain countries/languages or certain events. The text reminders are also defined here, as well as their translations.  This script editor will be replaced by a new version soon.", 
            "title": "Script editor"
        }, 
        {
            "location": "/components/#compilation", 
            "text": "The scripts and their accompanying data is compiled from the script editor into text files that can be read by the engine. This compilation process also includes \"numerical\" medical data, i.e. databases of drug interactions etc. Some of this data is bought from other companies, some of it is produced in-house.", 
            "title": "Compilation"
        }, 
        {
            "location": "/installation/", 
            "text": "Installation\n\n\nThe main steps to installing EBMeDS 2.0 are the following:\n\n\n\n\nInstall prerequisites\n\n\nGet a Docker registry username from Duodecim\n\n\nPull the Docker images\n\n\nConfigure the individual Docker images\n\n\nConfigure the Docker Swarm\n\n\nStart up the Swarm\n\n\n\n\nThese steps are given in more detail below.\n\n\nInstall prerequisites\n\n\nEBMeDS 2.x only requires Docker. Docker runs on Linux, Windows and Mac, but Linux is recommended, since Docker runs Linux internally and thus running it on other platforms incurs a small performance hit. Note that Docker supports running Windows internally on newer versions of Windows Server, but this is not supported by EBMeDS.\n\n\nInstall Docker\n\n\nThe installation instructions for Docker itself can be found on e.g. \nDocker's site\n. We support version 1.12+.\n\n\nInstall Node.js (optional)\n\n\nYou will need Node.js to run the \nnpm [...]\n commands below. You can also run docker commands manually, removing the need for Node. The commands are defined in the file \npackage.json\n.\n\n\nDownload ebmeds-docker repository\n\n\nDownload the zip file from \nGithub\n (the \"Clone or download\" button) or if you have Git installed, run the \ngit clone\n command.\n\n\n# Get a copy of the EBMeDS Docker configuration\ngit clone https://github.com/ebmeds/ebmeds-docker.git\n\n\n\n\nThis repository does not contain the Docker images themselves, only startup scripts and configuration files. It also contains a sample \ndocker-compose.yml\n file to get a Docker Swarm up and running with minimal effort.\n\n\nGet a Docker registry username from Duodecim\n\n\nIf you don't already have it, you need a Duodecim-supplied username and password to be able to download the EBMeDS Docker images, which reside in a private repository at \nquay.io\n. The username is in the form \nduodecim+yourname\n.\n\n\nLogin to quay.io and pull the required images\n\n\nThe built Docker images are stored in a repository on quay.io. Vendor organizations are provided with a login username and password. Developers with access to the EBMeDS Github repos can build the images locally.\n\n\nPull the Docker images\n\n\n# Go to the downloaded ebmeds-docker repository\ncd /path/to/ebmeds-docker\n\n# Run script that downloads the latest stable version of the images\n./get-images.sh\n\n# OR if one wishes to use e.g. the latest unstable version\n./get-images.sh dev\n\n# OR if one wishes to use a specific old version (not recommended)\n./get-images.sh 2.0.1\n\n\n\n\nYou need the proper Docker images downloaded (\"pulled\") onto your server before running them. This is true for single-machine servers and for each node in larger clusters.\n\n\nThe \nget-images.sh\n script will ask for the username/password of the EBMeDS Docker registry located at \nquay.io\n. These credentials are supplied by Duodecim. It will then pull the appropriate docker images and tag them with the following names:\n\n\n\n\nengine\n\n\napi-gateway\n\n\nauth\n\n\ncoaching\n\n\nelastichsearch \n(vanilla)\n\n\nkibana \n(vanilla)\n\n\nlogstash \n(vanilla)\n\n\n\n\nThe last three images are the vanilla ELK stack, the rest are custom images.\n\n\nDeployment\n\n\nDocker 1.13+\n\n\n# In the ebmeds-docker root directory:\nnpm run docker:init    # init Docker Swarm if not already running.\nnpm run docker:start\n\n# to stop\nnpm run docker:stop\n\n# and to restart:\nnpm run docker:restart  # same as stop + start\n\n# to stop the entire Swarm\nnpm run docker:deinit   # not needed in most cases\n\n\n\n\nAssuming that the Docker images are available on the machine, there are a bunch of NPM scripts in \nebmeds-docker\n that can start and stop a simple Docker Swarm configuration. \ndocker:init\n starts up a Docker Swarm with one member: the local machine. It is also the master of the swarm. The command outputs an ID number that other machines can use to join the swarm, for multi-node cluster support. See the Docker documentation for more details on this. The \ndocker:start\n and \ndocker:stop\n commands start and stop the containers themselves. They are configured in \ndocker-compose.yml\n.\n\n\nDocker 1.12\n\n\n# Example of how to start a service manually\ndocker swarm init\ndocker network create --driver overlay ebmedsnet\ndocker service create \\\n  --name api-gateway \\\n  -e LISTEN_PORT=3001 \\\n  -e ENGINE_URL='http://engine:3002/dss.asp?mode=test' \\\n  --network ebmedsnet \\\n  --publish 3001:3001 \\\n  --replicas=3 \\\n  --update-delay 10s \\\n  --update-parallelism 1 \\\n  api-gateway\ndocker service create ...\n\n\n\n\nThe oldest supported version of Docker does not have support for Docker Compose files when used together with Docker Swarm. Therefore the command \nnpm run docker:start\n will not work, and the \ndocker-compose.yml\n file must be transformed into e.g. shell scripts that set up the services manually. For example, starting the \napi-gateway\n service manually (see the example) is a matter of setting environment variables, publishing a port, setting the number of replicas and optionally setting some update settings for Docker's rolling updates.\n\n\nBefore this the swarm must be initialized and the network ebmedsnet created (in this example). Also note that the environment variables used in this example are the ones found in \napi-gateway/config.env\n.\n\n\nConfiguration\n\n\nEnvironment variables\n\n\nAll configuration of the EBMeDS services (except the ELK stack) is done through environment variables. These are defined in the \nebmeds-docker\n directory at \nimage-name\n/config.env\n, i.e. each container is configured separately. The \n.env\n files contain comments describing the different options.\n\n\nDefault ports\n\n\nThe containers inside the Swarm are configured to listen to the following ports per default:\n\n\n\n\napi-gateway: 3001\n\n\nengine: 3002\n\n\ncoaching: 3003\n\n\nelasticsearch: 9200 (REST API), 9300 (node communication in clusters)\n\n\nlogstash: 5000 (TCP input)\n\n\nkibana: 5601\n\n\n\n\n\nThe above ports are not accessible outside of the swarm, except for \napi-gateway\n at port 3001 and \nkibana\n at port 5601. Port 3001 should therefore be open for general EBMeDS usage (see #Usage) and port 5601 is for log data analysis using the web UI in Kibana, which should be accessible only by trusted sources.\n\n\n\n\nFile system access\n\n\nDocker containers cannot per default persist data to disk, i.e. all changes to the container file system are lost when the container is shut down. Therefore, to be able to save data between restarts, the host system running Docker must mount its own directories on top of the file system inside of Docker, to \"catch\" the saved data. The table below lists the folders that are configured to be mounted onto various Docker containers ($ROOT is the path to the \nebmeds-docker\n project):\n\n\n\n\n\n\n\n\nDefault host directory (configurable)\n\n\nInternal Docker directory\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n$ROOT/elasticsearch/data\n\n\n/usr/share/elasticsearch/data\n\n\nThe place where the Elasticsearch DB data lives (may get very large)\n\n\n\n\n\n\n$ROOT/logstash/pipeline\n\n\n/usr/share/logstash/pipeline\n\n\nHow Logstash should process the data flowing through it\n\n\n\n\n\n\n$ROOT/logstash/queue\n\n\n/usr/share/logstash/data/queue\n\n\nInstead of storing the input queue in memory, persist it to disk to avoid data loss\n\n\n\n\n\n\n$ROOT/logstash/config\n\n\n/usr/share/logstash/config\n\n\nGeneral configuration for the Logstash server process (usually no need to edit this)\n\n\n\n\n\n\n$ROOT/kibana/config/\n\n\n/usr/share/kibana/config\n\n\nGeneral configuration for the Kibana server process (usually no need to edit this)\n\n\n\n\n\n\n\n\nNote that the permissions must be set correctly for the host folders: the UID and GID should match the UID and GID that the user inside Docker has (which per default is 1000/1000). SELinux labels can also be used to avoid getting permission errors. Permissions on Windows/OSX will behave differently.\n\n\n\nAlso note that the Docker daemon itself will use the file system to store pulled images internally. These images are not removed by default when new versions are pulled, to support roll-backs of updates that are not working etc. The system administrator should keep in mind that the images will, in time, take up a lot of disk space, and steps should be taken to remove unused Docker images.\n\n\n\n\nAdvanced deployment\n\n\nThere are many advanced features of Docker that can be used to make the service more failsafe. This includes backups, monitoring, automatic node failure recovery etc. These advanced features should be implemented by a competent DevOps professional and is not covered in this documentation.\n\n\nVerifying the installation\n\n\nTo see that the installation is succesful, perform the checklist below. $HOST is the hostname of the server running the Swarm. It may also be localhost:\n\n\n\n\nMake a HTTP GET to $HOST:3001/status, you should receive the reply \"OK\".\n\n\nMake a HTTP POST to $HOST:3001/xml, with the payload of an EBMeDS XML request. You should receive an XML response.\n\n\nOpen up Kibana at $HOST:5601 and add the indexes \nlogstash\n and \nengine\n, if not already added.\n\n\nIn the \"Discover\" tab, check the \nlogstash\n index to see that logging from the various containers is working.\n\n\nIn the \"Discover\" tab, check the \nengine\n index to see that logging of request/response pairs is working (this kind of logging can be turned off).\n\n\nOn the host's filesystem, check that \n$ROOT/elasticsearch/data\n and \n$ROOT/logstash/queue\n (or what you have configured them to be) are being populated with files. If not, the folder mounts are not working, and any data saved to the Elasticsearch database will disappear when the \nelasticsearch\n container is stopped.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#installation", 
            "text": "The main steps to installing EBMeDS 2.0 are the following:   Install prerequisites  Get a Docker registry username from Duodecim  Pull the Docker images  Configure the individual Docker images  Configure the Docker Swarm  Start up the Swarm   These steps are given in more detail below.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#install-prerequisites", 
            "text": "EBMeDS 2.x only requires Docker. Docker runs on Linux, Windows and Mac, but Linux is recommended, since Docker runs Linux internally and thus running it on other platforms incurs a small performance hit. Note that Docker supports running Windows internally on newer versions of Windows Server, but this is not supported by EBMeDS.", 
            "title": "Install prerequisites"
        }, 
        {
            "location": "/installation/#install-docker", 
            "text": "The installation instructions for Docker itself can be found on e.g.  Docker's site . We support version 1.12+.", 
            "title": "Install Docker"
        }, 
        {
            "location": "/installation/#install-nodejs-optional", 
            "text": "You will need Node.js to run the  npm [...]  commands below. You can also run docker commands manually, removing the need for Node. The commands are defined in the file  package.json .", 
            "title": "Install Node.js (optional)"
        }, 
        {
            "location": "/installation/#download-ebmeds-docker-repository", 
            "text": "Download the zip file from  Github  (the \"Clone or download\" button) or if you have Git installed, run the  git clone  command.  # Get a copy of the EBMeDS Docker configuration\ngit clone https://github.com/ebmeds/ebmeds-docker.git  This repository does not contain the Docker images themselves, only startup scripts and configuration files. It also contains a sample  docker-compose.yml  file to get a Docker Swarm up and running with minimal effort.", 
            "title": "Download ebmeds-docker repository"
        }, 
        {
            "location": "/installation/#get-a-docker-registry-username-from-duodecim", 
            "text": "If you don't already have it, you need a Duodecim-supplied username and password to be able to download the EBMeDS Docker images, which reside in a private repository at  quay.io . The username is in the form  duodecim+yourname .", 
            "title": "Get a Docker registry username from Duodecim"
        }, 
        {
            "location": "/installation/#login-to-quayio-and-pull-the-required-images", 
            "text": "The built Docker images are stored in a repository on quay.io. Vendor organizations are provided with a login username and password. Developers with access to the EBMeDS Github repos can build the images locally.", 
            "title": "Login to quay.io and pull the required images"
        }, 
        {
            "location": "/installation/#pull-the-docker-images", 
            "text": "# Go to the downloaded ebmeds-docker repository\ncd /path/to/ebmeds-docker\n\n# Run script that downloads the latest stable version of the images\n./get-images.sh\n\n# OR if one wishes to use e.g. the latest unstable version\n./get-images.sh dev\n\n# OR if one wishes to use a specific old version (not recommended)\n./get-images.sh 2.0.1  You need the proper Docker images downloaded (\"pulled\") onto your server before running them. This is true for single-machine servers and for each node in larger clusters.  The  get-images.sh  script will ask for the username/password of the EBMeDS Docker registry located at  quay.io . These credentials are supplied by Duodecim. It will then pull the appropriate docker images and tag them with the following names:   engine  api-gateway  auth  coaching  elastichsearch  (vanilla)  kibana  (vanilla)  logstash  (vanilla)   The last three images are the vanilla ELK stack, the rest are custom images.", 
            "title": "Pull the Docker images"
        }, 
        {
            "location": "/installation/#deployment", 
            "text": "", 
            "title": "Deployment"
        }, 
        {
            "location": "/installation/#docker-113", 
            "text": "# In the ebmeds-docker root directory:\nnpm run docker:init    # init Docker Swarm if not already running.\nnpm run docker:start\n\n# to stop\nnpm run docker:stop\n\n# and to restart:\nnpm run docker:restart  # same as stop + start\n\n# to stop the entire Swarm\nnpm run docker:deinit   # not needed in most cases  Assuming that the Docker images are available on the machine, there are a bunch of NPM scripts in  ebmeds-docker  that can start and stop a simple Docker Swarm configuration.  docker:init  starts up a Docker Swarm with one member: the local machine. It is also the master of the swarm. The command outputs an ID number that other machines can use to join the swarm, for multi-node cluster support. See the Docker documentation for more details on this. The  docker:start  and  docker:stop  commands start and stop the containers themselves. They are configured in  docker-compose.yml .", 
            "title": "Docker 1.13+"
        }, 
        {
            "location": "/installation/#docker-112", 
            "text": "# Example of how to start a service manually\ndocker swarm init\ndocker network create --driver overlay ebmedsnet\ndocker service create \\\n  --name api-gateway \\\n  -e LISTEN_PORT=3001 \\\n  -e ENGINE_URL='http://engine:3002/dss.asp?mode=test' \\\n  --network ebmedsnet \\\n  --publish 3001:3001 \\\n  --replicas=3 \\\n  --update-delay 10s \\\n  --update-parallelism 1 \\\n  api-gateway\ndocker service create ...  The oldest supported version of Docker does not have support for Docker Compose files when used together with Docker Swarm. Therefore the command  npm run docker:start  will not work, and the  docker-compose.yml  file must be transformed into e.g. shell scripts that set up the services manually. For example, starting the  api-gateway  service manually (see the example) is a matter of setting environment variables, publishing a port, setting the number of replicas and optionally setting some update settings for Docker's rolling updates.  Before this the swarm must be initialized and the network ebmedsnet created (in this example). Also note that the environment variables used in this example are the ones found in  api-gateway/config.env .", 
            "title": "Docker 1.12"
        }, 
        {
            "location": "/installation/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/installation/#environment-variables", 
            "text": "All configuration of the EBMeDS services (except the ELK stack) is done through environment variables. These are defined in the  ebmeds-docker  directory at  image-name /config.env , i.e. each container is configured separately. The  .env  files contain comments describing the different options.", 
            "title": "Environment variables"
        }, 
        {
            "location": "/installation/#default-ports", 
            "text": "The containers inside the Swarm are configured to listen to the following ports per default:   api-gateway: 3001  engine: 3002  coaching: 3003  elasticsearch: 9200 (REST API), 9300 (node communication in clusters)  logstash: 5000 (TCP input)  kibana: 5601   \nThe above ports are not accessible outside of the swarm, except for  api-gateway  at port 3001 and  kibana  at port 5601. Port 3001 should therefore be open for general EBMeDS usage (see #Usage) and port 5601 is for log data analysis using the web UI in Kibana, which should be accessible only by trusted sources.", 
            "title": "Default ports"
        }, 
        {
            "location": "/installation/#file-system-access", 
            "text": "Docker containers cannot per default persist data to disk, i.e. all changes to the container file system are lost when the container is shut down. Therefore, to be able to save data between restarts, the host system running Docker must mount its own directories on top of the file system inside of Docker, to \"catch\" the saved data. The table below lists the folders that are configured to be mounted onto various Docker containers ($ROOT is the path to the  ebmeds-docker  project):     Default host directory (configurable)  Internal Docker directory  Description      $ROOT/elasticsearch/data  /usr/share/elasticsearch/data  The place where the Elasticsearch DB data lives (may get very large)    $ROOT/logstash/pipeline  /usr/share/logstash/pipeline  How Logstash should process the data flowing through it    $ROOT/logstash/queue  /usr/share/logstash/data/queue  Instead of storing the input queue in memory, persist it to disk to avoid data loss    $ROOT/logstash/config  /usr/share/logstash/config  General configuration for the Logstash server process (usually no need to edit this)    $ROOT/kibana/config/  /usr/share/kibana/config  General configuration for the Kibana server process (usually no need to edit this)     Note that the permissions must be set correctly for the host folders: the UID and GID should match the UID and GID that the user inside Docker has (which per default is 1000/1000). SELinux labels can also be used to avoid getting permission errors. Permissions on Windows/OSX will behave differently.  \nAlso note that the Docker daemon itself will use the file system to store pulled images internally. These images are not removed by default when new versions are pulled, to support roll-backs of updates that are not working etc. The system administrator should keep in mind that the images will, in time, take up a lot of disk space, and steps should be taken to remove unused Docker images.", 
            "title": "File system access"
        }, 
        {
            "location": "/installation/#advanced-deployment", 
            "text": "There are many advanced features of Docker that can be used to make the service more failsafe. This includes backups, monitoring, automatic node failure recovery etc. These advanced features should be implemented by a competent DevOps professional and is not covered in this documentation.", 
            "title": "Advanced deployment"
        }, 
        {
            "location": "/installation/#verifying-the-installation", 
            "text": "To see that the installation is succesful, perform the checklist below. $HOST is the hostname of the server running the Swarm. It may also be localhost:   Make a HTTP GET to $HOST:3001/status, you should receive the reply \"OK\".  Make a HTTP POST to $HOST:3001/xml, with the payload of an EBMeDS XML request. You should receive an XML response.  Open up Kibana at $HOST:5601 and add the indexes  logstash  and  engine , if not already added.  In the \"Discover\" tab, check the  logstash  index to see that logging from the various containers is working.  In the \"Discover\" tab, check the  engine  index to see that logging of request/response pairs is working (this kind of logging can be turned off).  On the host's filesystem, check that  $ROOT/elasticsearch/data  and  $ROOT/logstash/queue  (or what you have configured them to be) are being populated with files. If not, the folder mounts are not working, and any data saved to the Elasticsearch database will disappear when the  elasticsearch  container is stopped.", 
            "title": "Verifying the installation"
        }, 
        {
            "location": "/api/fhir-stu3/", 
            "text": "FHIR STU3 API\n\n\nGetting started\n\n\nEBMeDS can be used with the \nHL7 FHIR STU3\n format. The main unit of information in FHIR is the \nresource\n, which is a JSON object with a particular type. FHIR can also be expressed in XML form, but this is not supported by EBMeDS.\n\n\nOne can interact with EBMeDS using FHIR in the following ways:\n\n\n\n\nUsing a custom REST interface to retrieve static FHIR resources (Questionnaire, PlanDefinition, Goal)\n\n\nUsing \nCDS Hooks\n to call decision support. Input and output resource types depend on the hook.\n\n\n\n\nCustom REST API", 
            "title": "FHIR STU3"
        }, 
        {
            "location": "/api/fhir-stu3/#fhir-stu3-api", 
            "text": "", 
            "title": "FHIR STU3 API"
        }, 
        {
            "location": "/api/fhir-stu3/#getting-started", 
            "text": "EBMeDS can be used with the  HL7 FHIR STU3  format. The main unit of information in FHIR is the  resource , which is a JSON object with a particular type. FHIR can also be expressed in XML form, but this is not supported by EBMeDS.  One can interact with EBMeDS using FHIR in the following ways:   Using a custom REST interface to retrieve static FHIR resources (Questionnaire, PlanDefinition, Goal)  Using  CDS Hooks  to call decision support. Input and output resource types depend on the hook.", 
            "title": "Getting started"
        }, 
        {
            "location": "/api/fhir-stu3/#custom-rest-api", 
            "text": "", 
            "title": "Custom REST API"
        }, 
        {
            "location": "/api/xml/", 
            "text": "", 
            "title": "XML"
        }
    ]
}